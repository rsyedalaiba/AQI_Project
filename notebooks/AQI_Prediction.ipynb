{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvpwIx9cf80h",
        "outputId": "32a61474-1519-47e7-ae89-deee268e87f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder structure created successfully!\n",
            "\n",
            "Project Root: /content\n",
            "Data Path: /content/AQI_Project/data/KARACHI_AQI_WEATHER_2023_TO_2025.csv\n",
            "\n",
            "Folder Tree:\n",
            " - AQI_Project/data\n",
            " - AQI_Project/models\n",
            " - AQI_Project/notebooks\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "project_root = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
        "folders = ['AQI_Project/data', 'AQI_Project/models', 'AQI_Project/notebooks']\n",
        "\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(project_root, folder), exist_ok=True)\n",
        "\n",
        "DATA_PATH = os.path.join(project_root, 'AQI_Project', 'data', 'KARACHI_AQI_WEATHER_2023_TO_2025.csv')\n",
        "\n",
        "print(\"Folder structure created successfully!\\n\")\n",
        "print(\"Project Root:\", project_root)\n",
        "print(\"Data Path:\", DATA_PATH)\n",
        "print(\"\\nFolder Tree:\")\n",
        "for folder in folders:\n",
        "    print(\" -\", folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"AQI_Project/data/ENGINEERED.csv\"\n",
        "df_recent = pd.read_csv(data_path)\n",
        "df_recent.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwgwAozE7kW_",
        "outputId": "b8122fbb-77f7-4cbd-dfc9-ac951c18e32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['time', 'pm10', 'pm2_5', 'carbon_monoxide', 'nitrogen_dioxide',\n",
              "       'sulphur_dioxide', 'ozone', 'us_aqi', 'temperature_2m',\n",
              "       'relative_humidity_2m', 'wind_speed_10m', 'day', 'month', 'year',\n",
              "       'day_of_week', 'temp_roll24', 'humidity_roll24', 'wind_roll24',\n",
              "       'AQI_roll24', 'AQI_roll_std24', 'AQI_lag24', 'AQI_roll3',\n",
              "       'AQI_trend_24h'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# 1. Load recent hourly data\n",
        "data_path = \"AQI_Project/data/ENGINEERED.csv\"\n",
        "df_recent = pd.read_csv(data_path)\n",
        "\n",
        "# 2. Ensure datetime column is proper (agar hai)\n",
        "if 'time' in df_recent.columns:\n",
        "    df_recent['time'] = pd.to_datetime(df_recent['time'])\n",
        "\n",
        "# 3. Select only the 15 trained features\n",
        "trained_features = [\n",
        "    'pm10', 'pm2_5', 'us_aqi', 'day', 'month', 'year',\n",
        "    'day_of_week', 'temp_roll24', 'humidity_roll24', 'wind_roll24',\n",
        "    'AQI_roll24', 'AQI_roll_std24', 'AQI_lag24', 'AQI_roll3', 'AQI_trend_24h'\n",
        "]\n",
        "\n",
        "X_recent = df_recent[trained_features]\n",
        "\n",
        "# 4. Select last 24 rows only\n",
        "X_last_24 = X_recent.tail(18)\n",
        "time_last_24 = df_recent['time'].tail(18) if 'time' in df_recent.columns else range(18)\n",
        "\n",
        "# 5. Load trained multi-output LightGBM model\n",
        "model_path = \"AQI_Project/models/best_lightgbm_multioutput.pkl\"\n",
        "lgbm_model = joblib.load(model_path)\n",
        "\n",
        "# 6. Make predictions on last 24 hours\n",
        "predictions = lgbm_model.predict(X_last_24)\n",
        "\n",
        "# 7. Convert to DataFrame for readability\n",
        "pred_df = pd.DataFrame(predictions, columns=['AQI_24h', 'AQI_48h', 'AQI_72h'])\n",
        "pred_df['time'] = time_last_24.values\n",
        "\n",
        "# 8. Show results\n",
        "print(\"\\n Predictions for last 18 hours:\")\n",
        "print(pred_df)\n",
        "\n",
        "# 9. Optional: save predictions\n",
        "pred_df.to_csv(\"AQI_Project/data/recent_predictions_24h.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcTKeyoh8FbY",
        "outputId": "6a6ce393-4cee-4d1c-fed7-936fcfcca706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Predictions for last 18 hours:\n",
            "     AQI_24h   AQI_48h   AQI_72h                time\n",
            "0   0.845124 -0.458906  0.433087 2025-11-01 01:00:00\n",
            "1   0.798093 -0.471503  0.466497 2025-11-01 02:00:00\n",
            "2   0.795671 -0.509284  0.471349 2025-11-01 03:00:00\n",
            "3   0.498298 -0.474404  0.383777 2025-11-01 04:00:00\n",
            "4   0.338871 -0.534999  0.350655 2025-11-01 05:00:00\n",
            "5   0.317229 -0.563228  0.345803 2025-11-01 06:00:00\n",
            "6   0.476656 -0.447580  0.378924 2025-11-01 07:00:00\n",
            "7   0.784219 -0.455768  0.472780 2025-11-01 08:00:00\n",
            "8   0.850306 -0.475203  0.472780 2025-11-01 09:00:00\n",
            "9   0.457439 -0.504507  0.361096 2025-11-01 10:00:00\n",
            "10  0.463472 -0.049238  0.352577 2025-11-01 11:00:00\n",
            "11  0.444384 -0.067781  0.340733 2025-11-01 12:00:00\n",
            "12  0.265815 -0.077725  0.203927 2025-11-01 13:00:00\n",
            "13  0.364651  0.058207  0.206945 2025-11-01 14:00:00\n",
            "14  0.288232  0.025326  0.194943 2025-11-01 15:00:00\n",
            "15  0.232311  0.105160  0.155550 2025-11-01 16:00:00\n",
            "16  0.364707  0.091352  0.249561 2025-11-01 17:00:00\n",
            "17  0.267728 -0.372428  0.320825 2025-11-01 18:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIRST PERFORMING INVERSE TRANSFORMATION TO GET REAL AQI VALUES (TESTING)**"
      ],
      "metadata": {
        "id": "VFYQWPd-TVIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# üìò AQI Prediction with Inverse Transform\n",
        "# ==============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Load the engineered dataset (already transformed)\n",
        "data_path = \"AQI_Project/data/ENGINEERED.csv\"\n",
        "df_recent = pd.read_csv(data_path)\n",
        "\n",
        "# Select only the 15 features used during model training\n",
        "selected_features = [\n",
        "    'pm10', 'pm2_5', 'us_aqi', 'day', 'month', 'year',\n",
        "    'day_of_week', 'temp_roll24', 'humidity_roll24', 'wind_roll24',\n",
        "    'AQI_roll24', 'AQI_roll_std24', 'AQI_lag24', 'AQI_roll3', 'AQI_trend_24h'\n",
        "]\n",
        "\n",
        "X_recent = df_recent[selected_features].copy()\n",
        "\n",
        "# Keep only the last 24 rows (1 day)\n",
        "X_recent = X_recent.tail(24)\n",
        "\n",
        "# Load your trained LightGBM model\n",
        "model_path = \"AQI_Project/models/best_lightgbm_multioutput.pkl\"\n",
        "lgbm_model = joblib.load(model_path)\n",
        "\n",
        "# Make predictions (transformed scale)\n",
        "predictions = lgbm_model.predict(X_recent)\n",
        "\n",
        "# Fit a new PowerTransformer on 'us_aqi' column of the CLEANED dataset\n",
        "cleaned_path = \"AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-CLEANED.csv\"\n",
        "cleaned_df = pd.read_csv(cleaned_path)\n",
        "\n",
        "pt_target = PowerTransformer(method='yeo-johnson')\n",
        "pt_target.fit(cleaned_df[['us_aqi']])\n",
        "\n",
        "# Inverse transform predictions for each output (24h, 48h, 72h)\n",
        "preds_24 = pt_target.inverse_transform(predictions[:, 0].reshape(-1, 1)).flatten()\n",
        "preds_48 = pt_target.inverse_transform(predictions[:, 1].reshape(-1, 1)).flatten()\n",
        "preds_72 = pt_target.inverse_transform(predictions[:, 2].reshape(-1, 1)).flatten()\n",
        "\n",
        "# Combine all results into a single DataFrame\n",
        "pred_df = pd.DataFrame({\n",
        "    'time': df_recent['time'].tail(24).values if 'time' in df_recent.columns else np.arange(len(preds_24)),\n",
        "    'AQI_24h': np.round(preds_24, 0),\n",
        "    'AQI_48h': np.round(preds_48, 0),\n",
        "    'AQI_72h': np.round(preds_72, 0)\n",
        "})\n",
        "\n",
        "# Display and save results\n",
        "print(\"\\nPredicted AQI values (inverse-transformed):\")\n",
        "print(pred_df.head(10))\n",
        "\n",
        "# Optional: Save predictions\n",
        "pred_df.to_csv(\"AQI_Project/data/recent_predictions_inverse.csv\", index=False)\n",
        "print(\"\\nPredictions saved successfully at: AQI_Project/data/recent_predictions_inverse.csv\")\n",
        "\n",
        "# Check realistic range\n",
        "print(\"\\nCheck range comparison:\")\n",
        "print(\"Cleaned AQI range:\", cleaned_df['us_aqi'].min(), \"-\", cleaned_df['us_aqi'].max())\n",
        "print(\"Predicted AQI range:\", pred_df[['AQI_24h', 'AQI_48h', 'AQI_72h']].min().min(), \"-\", pred_df[['AQI_24h', 'AQI_48h', 'AQI_72h']].max().max())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqaL9JLSFpMh",
        "outputId": "da909414-6f58-4452-e260-1eee123bd243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Predicted AQI values (inverse-transformed):\n",
            "                  time  AQI_24h  AQI_48h  AQI_72h\n",
            "0  2025-10-31 19:00:00    105.0     88.0     97.0\n",
            "1  2025-10-31 20:00:00    108.0     89.0     98.0\n",
            "2  2025-10-31 21:00:00    109.0     88.0     98.0\n",
            "3  2025-10-31 22:00:00    111.0     88.0     98.0\n",
            "4  2025-10-31 23:00:00    110.0     87.0     99.0\n",
            "5  2025-11-01 00:00:00     97.0     72.0     88.0\n",
            "6  2025-11-01 01:00:00    102.0     72.0     91.0\n",
            "7  2025-11-01 02:00:00    101.0     72.0     92.0\n",
            "8  2025-11-01 03:00:00    101.0     71.0     92.0\n",
            "9  2025-11-01 04:00:00     93.0     72.0     90.0\n",
            "\n",
            "üìÅ Predictions saved successfully at: AQI_Project/data/recent_predictions_inverse.csv\n",
            "\n",
            "üîç Check range comparison:\n",
            "Cleaned AQI range: 35 - 143\n",
            "Predicted AQI range: 70.0 - 111.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THE INVERSE-TRANSFORM.PKL FILE IS SAVED TO CONSISTENTLY RESTORE PREDICTED VALUES TO THEIR ORIGINAL AQI SCALE, PREVENTING DECIMAL OR NEGATIVE OUTPUTS IN FUTURE PREDICTIONS.**"
      ],
      "metadata": {
        "id": "Bz4NP_12P8Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== AQI prediction: inverse-transform targets =====\n",
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Paths\n",
        "engineered_path = \"AQI_Project/data/ENGINEERED.csv\"   # already transformed & engineered (15 cols)\n",
        "model_path = \"AQI_Project/models/best_lightgbm_multioutput.pkl\"\n",
        "target_transformer_path = \"AQI_Project/models/yeo_target_only_us_aqi.pkl\"\n",
        "cleaned_path = \"AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-CLEANED.csv\"  # has raw us_aqi\n",
        "\n",
        "# 1) Load engineered prediction CSV (already transformed)\n",
        "df_eng = pd.read_csv(engineered_path)\n",
        "\n",
        "# 2) Keep required 15 features (these must be the same transformed features the model expects)\n",
        "selected_features = [\n",
        "    'pm10', 'pm2_5', 'us_aqi', 'day', 'month', 'year',\n",
        "    'day_of_week', 'temp_roll24', 'humidity_roll24', 'wind_roll24',\n",
        "    'AQI_roll24', 'AQI_roll_std24', 'AQI_lag24', 'AQI_roll3', 'AQI_trend_24h'\n",
        "]\n",
        "\n",
        "X = df_eng[selected_features].copy()\n",
        "\n",
        "# 3) Take last 18 rows for prediction\n",
        "X_last = X.tail(18).reset_index(drop=True)\n",
        "\n",
        "# 4) Load model and predict (these predictions are in transformed target-space)\n",
        "model = joblib.load(model_path)\n",
        "pred_transformed = model.predict(X_last)   # shape: (18, 3)\n",
        "\n",
        "# 5) Load or fit the target-only transformer (Yeo-Johnson fitted on raw us_aqi)\n",
        "if os.path.exists(target_transformer_path):\n",
        "    pt_target = joblib.load(target_transformer_path)\n",
        "    print(\" Loaded saved target transformer.\")\n",
        "else:\n",
        "    # Fit on the raw 'us_aqi' from cleaned dataset (this recreates a transformer for the target)\n",
        "    cleaned_df = pd.read_csv(cleaned_path)\n",
        "    if 'us_aqi' not in cleaned_df.columns:\n",
        "        raise ValueError(\"cleaned CSV does not contain 'us_aqi' column; cannot fit target transformer.\")\n",
        "    pt_target = PowerTransformer(method='yeo-johnson')\n",
        "    pt_target.fit(cleaned_df[['us_aqi']])\n",
        "    joblib.dump(pt_target, target_transformer_path)\n",
        "    print(\" Fitted and saved target transformer at:\", target_transformer_path)\n",
        "\n",
        "# 6) Inverse transform each predicted column back to AQI scale\n",
        "#    Predictions are shape (n_rows, 3). pt_target.inverse_transform expects 2D arrays.\n",
        "preds_24 = pt_target.inverse_transform(pred_transformed[:, 0].reshape(-1, 1)).flatten()\n",
        "preds_48 = pt_target.inverse_transform(pred_transformed[:, 1].reshape(-1, 1)).flatten()\n",
        "preds_72 = pt_target.inverse_transform(pred_transformed[:, 2].reshape(-1, 1)).flatten()\n",
        "\n",
        "# 7) Post-process: round to integers and clip negative values to 0 (AQI cannot be negative)\n",
        "preds_24 = np.round(preds_24).astype(int)\n",
        "preds_48 = np.round(preds_48).astype(int)\n",
        "preds_72 = np.round(preds_72).astype(int)\n",
        "\n",
        "preds_24 = np.clip(preds_24, 0, None)\n",
        "preds_48 = np.clip(preds_48, 0, None)\n",
        "preds_72 = np.clip(preds_72, 0, None)\n",
        "\n",
        "# 8) Build result DataFrame with timestamps if available\n",
        "times = df_eng['time'].tail(18).reset_index(drop=True) if 'time' in df_eng.columns else pd.RangeIndex(start=0, stop=len(preds_24))\n",
        "result = pd.DataFrame({\n",
        "    'time': times,\n",
        "    'AQI_24h': preds_24,\n",
        "    'AQI_48h': preds_48,\n",
        "    'AQI_72h': preds_72\n",
        "})\n",
        "\n",
        "# 9) Save and print\n",
        "out_path = \"AQI_Project/data/recent_predictions_inverse.csv\"\n",
        "result.to_csv(out_path, index=False)\n",
        "print(\"\\n Saved inverse-transformed predictions to:\", out_path)\n",
        "print(result.head(18))\n",
        "\n",
        "# 10) Quick sanity check: compare result range with cleaned data range\n",
        "cleaned_df = pd.read_csv(cleaned_path)\n",
        "print(\"\\nCleaned AQI range:\", cleaned_df['us_aqi'].min(), \"-\", cleaned_df['us_aqi'].max())\n",
        "print(\"Predicted AQI range:\", result[['AQI_24h','AQI_48h','AQI_72h']].min().min(), \"-\", result[['AQI_24h','AQI_48h','AQI_72h']].max().max())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvUjC4u3JxNa",
        "outputId": "93cc2096-3043-4cac-d534-ae7d61136662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fitted and saved target transformer at: AQI_Project/models/yeo_target_only_us_aqi.pkl\n",
            "\n",
            "‚úÖ Saved inverse-transformed predictions to: AQI_Project/data/recent_predictions_inverse.csv\n",
            "                  time  AQI_24h  AQI_48h  AQI_72h\n",
            "0  2025-11-01 01:00:00      102       72       91\n",
            "1  2025-11-01 02:00:00      101       72       92\n",
            "2  2025-11-01 03:00:00      101       71       92\n",
            "3  2025-11-01 04:00:00       93       72       90\n",
            "4  2025-11-01 05:00:00       89       71       89\n",
            "5  2025-11-01 06:00:00       88       70       89\n",
            "6  2025-11-01 07:00:00       92       72       90\n",
            "7  2025-11-01 08:00:00      101       72       92\n",
            "8  2025-11-01 09:00:00      103       72       92\n",
            "9  2025-11-01 10:00:00       92       71       89\n",
            "\n",
            "Cleaned AQI range: 35 - 143\n",
            "Predicted AQI range: 70 - 103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Saved inverse-transformed predictions to:\", out_path)\n",
        "print(result.head(18))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYnPnU94KALS",
        "outputId": "1cd22a07-1e77-474a-ffa0-f08b925d391c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Saved inverse-transformed predictions to: AQI_Project/data/recent_predictions_inverse.csv\n",
            "                   time  AQI_24h  AQI_48h  AQI_72h\n",
            "0   2025-11-01 01:00:00      102       72       91\n",
            "1   2025-11-01 02:00:00      101       72       92\n",
            "2   2025-11-01 03:00:00      101       71       92\n",
            "3   2025-11-01 04:00:00       93       72       90\n",
            "4   2025-11-01 05:00:00       89       71       89\n",
            "5   2025-11-01 06:00:00       88       70       89\n",
            "6   2025-11-01 07:00:00       92       72       90\n",
            "7   2025-11-01 08:00:00      101       72       92\n",
            "8   2025-11-01 09:00:00      103       72       92\n",
            "9   2025-11-01 10:00:00       92       71       89\n",
            "10  2025-11-01 11:00:00       92       80       89\n",
            "11  2025-11-01 12:00:00       92       80       89\n",
            "12  2025-11-01 13:00:00       87       80       86\n",
            "13  2025-11-01 14:00:00       90       82       86\n",
            "14  2025-11-01 15:00:00       88       82       86\n",
            "15  2025-11-01 16:00:00       86       84       85\n",
            "16  2025-11-01 17:00:00       90       83       87\n",
            "17  2025-11-01 18:00:00       87       74       88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTING PREDICTIONS WITH HOPSWORKS**"
      ],
      "metadata": {
        "id": "LAp7PpU0TBGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install hopsworks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XXfyCrdUMH2x",
        "outputId": "aea1535d-9b44-4188-8ce4-54f4a24abce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hopsworks\n",
            "  Downloading hopsworks-4.4.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pyhumps==1.6.1 (from hopsworks)\n",
            "  Downloading pyhumps-1.6.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from hopsworks) (2.32.4)\n",
            "Collecting furl (from hopsworks)\n",
            "  Downloading furl-2.1.4-py2.py3-none-any.whl.metadata (25 kB)\n",
            "Collecting boto3 (from hopsworks)\n",
            "  Downloading boto3-1.40.65-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.12/dist-packages (from pandas[mysql]<2.3.0->hopsworks) (2.2.2)\n",
            "Collecting numpy<2 (from hopsworks)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjks (from hopsworks)\n",
            "  Downloading pyjks-20.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mock (from hopsworks)\n",
            "  Downloading mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting avro==1.11.3 (from hopsworks)\n",
            "  Downloading avro-1.11.3.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyMySQL[rsa] (from hopsworks)\n",
            "  Downloading pymysql-1.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from hopsworks) (5.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from hopsworks) (2025.3.0)\n",
            "Collecting retrying (from hopsworks)\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting hopsworks_aiomysql==0.2.1 (from hopsworks_aiomysql[sa]==0.2.1->hopsworks)\n",
            "  Downloading hopsworks_aiomysql-0.2.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting opensearch-py<=2.4.2,>=1.1.0 (from hopsworks)\n",
            "  Downloading opensearch_py-2.4.2-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from hopsworks) (4.67.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.49.1 in /usr/local/lib/python3.12/dist-packages (from hopsworks) (1.76.0)\n",
            "Collecting protobuf<5.0.0,>=4.25.4 (from hopsworks)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hopsworks) (25.0)\n",
            "Collecting sqlalchemy<=2.0.29,>=1.3 (from hopsworks_aiomysql[sa]==0.2.1->hopsworks)\n",
            "  Downloading SQLAlchemy-2.0.29-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.49.1->hopsworks) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.12/dist-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2.9.0.post0)\n",
            "Requirement already satisfied: certifi>=2022.12.07 in /usr/local/lib/python3.12/dist-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2025.10.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->pandas[mysql]<2.3.0->hopsworks) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->pandas[mysql]<2.3.0->hopsworks) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->hopsworks) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->hopsworks) (3.11)\n",
            "Collecting botocore<1.41.0,>=1.40.65 (from boto3->hopsworks)\n",
            "  Downloading botocore-1.40.65-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->hopsworks)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->hopsworks)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting orderedmultidict>=1.0.1 (from furl->hopsworks)\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting javaobj-py3 (from pyjks->hopsworks)\n",
            "  Downloading javaobj_py3-0.4.4-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pyasn1>=0.3.5 in /usr/local/lib/python3.12/dist-packages (from pyjks->hopsworks) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.12/dist-packages (from pyjks->hopsworks) (0.4.2)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.12/dist-packages (from pyjks->hopsworks) (3.23.0)\n",
            "Collecting twofish (from pyjks->hopsworks)\n",
            "  Downloading twofish-0.3.0.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from PyMySQL[rsa]->hopsworks) (43.0.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<=2.0.29,>=1.3->hopsworks_aiomysql[sa]==0.2.1->hopsworks) (3.2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->PyMySQL[rsa]->hopsworks) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->PyMySQL[rsa]->hopsworks) (2.23)\n",
            "Downloading hopsworks-4.4.2-py3-none-any.whl (691 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m691.6/691.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hopsworks_aiomysql-0.2.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opensearch_py-2.4.2-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m258.6/258.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.65-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading furl-2.1.4-py2.py3-none-any.whl (27 kB)\n",
            "Downloading mock-5.2.0-py3-none-any.whl (31 kB)\n",
            "Downloading pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading botocore-1.40.65-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymysql-1.1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.29-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading javaobj_py3-0.4.4-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: avro, twofish\n",
            "  Building wheel for avro (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro: filename=avro-1.11.3-py2.py3-none-any.whl size=123962 sha256=ea9040dd7162715e3d99d5c4f12eeb06680ca00605aad6c6c3757b370f0f99bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/82/d3/8eb96fda033c7f1661086e2f8afb13f04817886d28b12f1e72\n",
            "  Building wheel for twofish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twofish: filename=twofish-0.3.0-cp312-cp312-linux_x86_64.whl size=24323 sha256=854d02f0041d9e9b5ad7dfaf67cefa4ae10e474f9433ef71c5d6d443c47a7926\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/81/02/abf836d4acb19a3de48f6bfd738cb9bcb762978b835bca2faa\n",
            "Successfully built avro twofish\n",
            "Installing collected packages: twofish, pyhumps, javaobj-py3, sqlalchemy, retrying, PyMySQL, protobuf, orderedmultidict, numpy, mock, jmespath, avro, pyjks, opensearch-py, hopsworks_aiomysql, furl, botocore, s3transfer, boto3, hopsworks\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.44\n",
            "    Uninstalling SQLAlchemy-2.0.44:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.44\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMySQL-1.1.2 avro-1.11.3 boto3-1.40.65 botocore-1.40.65 furl-2.1.4 hopsworks-4.4.2 hopsworks_aiomysql-0.2.1 javaobj-py3-0.4.4 jmespath-1.0.1 mock-5.2.0 numpy-1.26.4 opensearch-py-2.4.2 orderedmultidict-1.0.1 protobuf-4.25.8 pyhumps-1.6.1 pyjks-20.0.0 retrying-1.4.2 s3transfer-0.14.0 sqlalchemy-2.0.29 twofish-0.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "200d1d3a1b0a4af4a243f29eee265f58"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hopsworks\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# ---  Connect to Hopsworks\n",
        "project = hopsworks.login()\n",
        "fs = project.get_feature_store()\n",
        "\n",
        "# ---  Fetch data from ONLINE FEATURE STORE\n",
        "feature_group = fs.get_feature_group(name=\"aqi_features_engineered\", version=2)\n",
        "df_features = feature_group.read(read_options={\"online\": True})\n",
        "\n",
        "# ---  Convert 'time' column from integer epoch to datetime\n",
        "df_features['time'] = pd.to_datetime(df_features['time'], unit='s')\n",
        "\n",
        "# ---  Sort data in *ascending* order (oldest ‚Üí newest)\n",
        "df_features = df_features.sort_values(by='time', ascending=True)\n",
        "\n",
        "# --- Select latest 18 records for prediction (but keep ascending order)\n",
        "df_recent = df_features.tail(18).reset_index(drop=True)\n",
        "\n",
        "# --- Load saved transformer and model\n",
        "pt = joblib.load(\"AQI_Project/models/yeo_target_only_us_aqi.pkl\")\n",
        "model = joblib.load(\"AQI_Project/models/best_lightgbm_multioutput.pkl\")\n",
        "\n",
        "# --- Select the same 15 columns used for training\n",
        "selected_features = [\n",
        "    'pm10', 'pm2_5', 'us_aqi', 'day', 'month', 'year',\n",
        "    'day_of_week', 'temp_roll24', 'humidity_roll24', 'wind_roll24',\n",
        "    'aqi_roll24', 'aqi_roll_std24', 'aqi_lag24', 'aqi_roll3', 'aqi_trend_24h'\n",
        "]\n",
        "\n",
        "# ---  Ensure column order matches exactly those used during training\n",
        "X_recent = df_recent[selected_features].copy()\n",
        "\n",
        "# ---  Fix warning by ensuring PowerTransformer gets proper feature names\n",
        "X_recent.columns = selected_features\n",
        "\n",
        "print(\" Skipping input transformation ‚Äî data is already transformed and engineered.\")\n",
        "\n",
        "# --- Perform prediction\n",
        "predictions = model.predict(X_recent)\n",
        "\n",
        "# --- Inverse-transform target values if they were transformed before training\n",
        "try:\n",
        "    pt_target = joblib.load(\"AQI_Project/models/yeo_target_only_us_aqi.pkl\")\n",
        "    preds_24 = pt_target.inverse_transform(predictions[:, 0].reshape(-1, 1)).flatten()\n",
        "    preds_48 = pt_target.inverse_transform(predictions[:, 1].reshape(-1, 1)).flatten()\n",
        "    preds_72 = pt_target.inverse_transform(predictions[:, 2].reshape(-1, 1)).flatten()\n",
        "except:\n",
        "    preds_24, preds_48, preds_72 = predictions[:, 0], predictions[:, 1], predictions[:, 2]\n",
        "\n",
        "# --- Display results sorted in time order\n",
        "pred_df = pd.DataFrame({\n",
        "    'time': df_recent['time'].values,\n",
        "    'Predicted AQI(Next 24h)': np.round(preds_24, 0),\n",
        "    'Predicted AQI(Next 48h)': np.round(preds_48, 0),\n",
        "    'Predicted AQI(Next 72h)': np.round(preds_72, 0)\n",
        "})\n",
        "\n",
        "pred_df = pred_df.sort_values(by='time', ascending=True).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n Predicted AQI values (inverse-transformed):\")\n",
        "print(pred_df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaZT7yHTRmRc",
        "outputId": "5caf9030-d860-4d2d-fd6e-45ca7157b3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "UserWarning: The installed hopsworks client version 4.4.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
            "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection closed.\n",
            "\n",
            "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1256597\n",
            "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.25s) \n",
            "‚úÖ Skipping input transformation ‚Äî data is already transformed and engineered.\n",
            "\n",
            "‚úÖ Predicted AQI values (inverse-transformed):\n",
            "                 time  Predicted AQI(Next 24h)  Predicted AQI(Next 48h)  \\\n",
            "0 2025-11-01 01:00:00                    102.0                     72.0   \n",
            "1 2025-11-01 02:00:00                    101.0                     72.0   \n",
            "2 2025-11-01 03:00:00                    101.0                     71.0   \n",
            "3 2025-11-01 04:00:00                     93.0                     72.0   \n",
            "4 2025-11-01 05:00:00                     89.0                     71.0   \n",
            "5 2025-11-01 06:00:00                     88.0                     70.0   \n",
            "6 2025-11-01 07:00:00                     92.0                     72.0   \n",
            "7 2025-11-01 08:00:00                    101.0                     72.0   \n",
            "8 2025-11-01 09:00:00                    103.0                     72.0   \n",
            "9 2025-11-01 10:00:00                     92.0                     71.0   \n",
            "\n",
            "   Predicted AQI(Next 72h)  \n",
            "0                     91.0  \n",
            "1                     92.0  \n",
            "2                     92.0  \n",
            "3                     90.0  \n",
            "4                     89.0  \n",
            "5                     89.0  \n",
            "6                     90.0  \n",
            "7                     92.0  \n",
            "8                     92.0  \n",
            "9                     89.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n",
            "UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n",
            "UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Predicted AQI values (inverse-transformed):\")\n",
        "print(pred_df.head(19))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jelBLbPTRz2B",
        "outputId": "ee895506-df59-4fb2-85fd-8bbe72c32965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Predicted AQI values (inverse-transformed):\n",
            "                  time  Predicted AQI(Next 24h)  Predicted AQI(Next 48h)  \\\n",
            "0  2025-11-01 01:00:00                    102.0                     72.0   \n",
            "1  2025-11-01 02:00:00                    101.0                     72.0   \n",
            "2  2025-11-01 03:00:00                    101.0                     71.0   \n",
            "3  2025-11-01 04:00:00                     93.0                     72.0   \n",
            "4  2025-11-01 05:00:00                     89.0                     71.0   \n",
            "5  2025-11-01 06:00:00                     88.0                     70.0   \n",
            "6  2025-11-01 07:00:00                     92.0                     72.0   \n",
            "7  2025-11-01 08:00:00                    101.0                     72.0   \n",
            "8  2025-11-01 09:00:00                    103.0                     72.0   \n",
            "9  2025-11-01 10:00:00                     92.0                     71.0   \n",
            "10 2025-11-01 11:00:00                     92.0                     80.0   \n",
            "11 2025-11-01 12:00:00                     92.0                     80.0   \n",
            "12 2025-11-01 13:00:00                     87.0                     80.0   \n",
            "13 2025-11-01 14:00:00                     90.0                     82.0   \n",
            "14 2025-11-01 15:00:00                     88.0                     82.0   \n",
            "15 2025-11-01 16:00:00                     86.0                     84.0   \n",
            "16 2025-11-01 17:00:00                     90.0                     83.0   \n",
            "17 2025-11-01 18:00:00                     87.0                     74.0   \n",
            "\n",
            "    Predicted AQI(Next 72h)  \n",
            "0                      91.0  \n",
            "1                      92.0  \n",
            "2                      92.0  \n",
            "3                      90.0  \n",
            "4                      89.0  \n",
            "5                      89.0  \n",
            "6                      90.0  \n",
            "7                      92.0  \n",
            "8                      92.0  \n",
            "9                      89.0  \n",
            "10                     89.0  \n",
            "11                     89.0  \n",
            "12                     86.0  \n",
            "13                     86.0  \n",
            "14                     86.0  \n",
            "15                     85.0  \n",
            "16                     87.0  \n",
            "17                     88.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVING THE YEO-JOHNSON TRANSFORMER FOR ALL FEATURES.**"
      ],
      "metadata": {
        "id": "QQ5t7ZYnU3N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from scipy.stats import skew\n",
        "import joblib  #  For saving the transformer\n",
        "\n",
        "# Load data\n",
        "cleaned_df = pd.read_csv(\"AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-CLEANED.csv\")\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Before transformation\n",
        "df_before = cleaned_df[numeric_cols].copy()\n",
        "print(\"Skewness before:\\n\", df_before.apply(lambda x: round(skew(x.dropna()), 3)))\n",
        "\n",
        "# Apply Yeo‚ÄìJohnson transformation\n",
        "pt = PowerTransformer(method='yeo-johnson')\n",
        "df_after = pd.DataFrame(pt.fit_transform(df_before), columns=numeric_cols)\n",
        "\n",
        "#  Save the fitted transformer model\n",
        "joblib.dump(pt, \"AQI_Project/models/yeo_transformer.pkl\")\n",
        "print(\"\\nYeo‚ÄìJohnson transformer saved successfully!\")\n",
        "\n",
        "# Replace numeric columns with transformed data\n",
        "cleaned_df[numeric_cols] = df_after\n",
        "\n",
        "# Save transformed dataset\n",
        "transformed_csv = \"AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-TRANSFORMED.csv\"\n",
        "cleaned_df.to_csv(transformed_csv, index=False)\n",
        "print(\"\\nTransformed dataset saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py0VQkBW_Mkd",
        "outputId": "7bf056a2-933b-476a-fb4f-5c207f7bf628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skewness before:\n",
            " pm10                    0.814\n",
            "pm2_5                   0.862\n",
            "carbon_monoxide         1.161\n",
            "nitrogen_dioxide        1.117\n",
            "sulphur_dioxide         0.905\n",
            "ozone                   0.676\n",
            "us_aqi                  0.841\n",
            "temperature_2m         -0.574\n",
            "relative_humidity_2m   -0.638\n",
            "wind_speed_10m          0.600\n",
            "dtype: float64\n",
            "\n",
            "Yeo‚ÄìJohnson transformer saved successfully!\n",
            "\n",
            "Transformed dataset saved successfully!\n"
          ]
        }
      ]
    }
  ]
}