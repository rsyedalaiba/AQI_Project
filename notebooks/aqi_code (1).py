# -*- coding: utf-8 -*-
"""AQI_CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C46lZQnfL24izrbPSmwDw57uEBPt-7M6

**IMPORT IMPORTANT LIBRARIES**
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install requests_cache

# Commented out IPython magic to ensure Python compatibility.
# %pip install openmeteo-requests

# Commented out IPython magic to ensure Python compatibility.
# %pip install retry-requests

# Commented out IPython magic to ensure Python compatibility.
# %pip install hopsworks

# Commented out IPython magic to ensure Python compatibility.
# %pip install confluent-kafka

pip install shap

""" **CREATE FOLDERS**"""

import os

project_root = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
folders = ['AQI_Project/data', 'AQI_Project/models', 'AQI_Project/notebooks']

for folder in folders:
    os.makedirs(os.path.join(project_root, folder), exist_ok=True)
    #open(os.path.join(folder, '.gitkeep'), 'a').close()

DATA_PATH = os.path.join(project_root, 'AQI_Project', 'data', 'KARACHI-AQI-RECORDS-2023-TO-2025.csv')

print("Folder structure created successfully!\n")
#print("Project Root:", project_root)
#print("Data Path:", DATA_PATH)
print("\nFolder Tree:")
for folder in folders:
    print(" -", folder)

# QUICK CHCEK IF DIRECTORIES EXISTS
import os

RAW_CSV = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-TO-2025.csv"
CLEANED_CSV = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-CLEANED.csv"
TRANSFORMED_CSV = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-TRANSFORMED.csv"
ENGINEERED_CSV = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-ENGINEERED.csv"

for path in [RAW_CSV, CLEANED_CSV, TRANSFORMED_CSV, ENGINEERED_CSV]:
    output_dir = os.path.dirname(path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Directory '{output_dir}' created.")
    else:
        print(f"Directory '{output_dir}' already exists.")

import pandas as pd
df = pd.read_csv("AQI_Project/data/KARACHI-AQI-RECORDS-2023-TO-2025.csv")
df = pd.read_csv("AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-CLEANED.csv")
df = pd.read_csv("AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-TRANSFORMED.csv")
df = pd.read_csv("AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-ENGINEERED.csv")

"""**FETCHING DATA USING API + 2 YEARS DATA HISTORICAL BACKFILL**

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.preprocessing import StandardScaler
import os

import requests
import pandas as pd
import openmeteo_requests
import requests_cache
from retry_requests import retry
from datetime import datetime
import os

# CONFIGURATION

LAT, LON = 24.8607, 67.0011
START_DATE = "2023-09-30"
END_DATE = datetime.today().strftime('%Y-%m-%d')
OUTPUT_CSV = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-TO-2025.csv"
TIMEZONE = "auto"

print(f"Fetching data from {START_DATE} → {END_DATE}")

# FETCH AQI DATA

print("Fetching air quality data...")

aqi_url = "https://air-quality-api.open-meteo.com/v1/air-quality"
aqi_params = {
    "latitude": LAT,
    "longitude": LON,
    "hourly": "pm10,pm2_5,carbon_monoxide,nitrogen_dioxide,sulphur_dioxide,ozone,us_aqi",
    "start_date": START_DATE,
    "end_date": END_DATE,
    "timezone": TIMEZONE
}

aqi_response = requests.get(aqi_url, params=aqi_params)
aqi_response.raise_for_status()
aqi_data = aqi_response.json()

aqi_df = pd.DataFrame(aqi_data["hourly"])
aqi_df["time"] = pd.to_datetime(aqi_df["time"]).dt.tz_localize(None)
print(f"AQI records fetched: {len(aqi_df)}")

# FETCH WEATHER DATA

print("Fetching weather data...")

cache_session = requests_cache.CachedSession('.cache', expire_after=-1)
retry_session = retry(cache_session, retries=5, backoff_factor=0.3)
openmeteo = openmeteo_requests.Client(session=retry_session)

weather_url = "https://archive-api.open-meteo.com/v1/archive"
weather_params = {
    "latitude": LAT,
    "longitude": LON,
    "start_date": START_DATE,
    "end_date": END_DATE,
    "hourly": ["temperature_2m", "relative_humidity_2m", "wind_speed_10m"],
    "timezone": TIMEZONE
}

responses = openmeteo.weather_api(weather_url, params=weather_params)
response = responses[0]
hourly = response.Hourly()

weather_data = {
    "time": pd.date_range(
        start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
        end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
        freq=pd.Timedelta(seconds=hourly.Interval()),
        inclusive="left"
    ),
    "temperature_2m": hourly.Variables(0).ValuesAsNumpy(),
    "relative_humidity_2m": hourly.Variables(1).ValuesAsNumpy(),
    "wind_speed_10m": hourly.Variables(2).ValuesAsNumpy()
}

weather_df = pd.DataFrame(weather_data)
weather_df["time"] = pd.to_datetime(weather_df["time"]).dt.tz_localize(None)
print(f"Weather records fetched: {len(weather_df)}")

# MERGE AQI + WEATHER DATA

print("Merging air quality and weather data...")
combined_df = pd.merge(aqi_df, weather_df, on="time", how="inner")
print(f"Combined records: {len(combined_df)}")

# SAVE NEW CSV

print("Saving combined data to CSV...")
combined_df.sort_values(by="time", inplace=True)

# Extracting the directory path from the output file path
output_dir = os.path.dirname(OUTPUT_CSV)

# Creating the directory if it doesn't exist
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

combined_df.to_csv(OUTPUT_CSV, index=False)

print(f"\n All data saved to : {OUTPUT_CSV}")
print(f" Total records: {len(combined_df)}")

# PREVIEW

print("\nLast few records:")
print(combined_df.tail())

"""**EDA - EXPLORATORY DATA ANALYSIS**"""

df = pd.read_csv(OUTPUT_CSV)
df.head()

print(df.tail())
print("\nDataset shape:", df.shape)

print("\nDataset Columns:")
print(df.columns)

print("\nDataset Info:")
print(df.info())

print("\nSummary Statistics:")
print(df.describe())

print("\nMissing values in each column:")
print(df.isnull().sum())

# Converting date column
df['time'] = pd.to_datetime(df['time'])
df = df.sort_values('time').reset_index(drop=True)

# Shape and summary
print("Rows:", df.shape[0], "Columns:", df.shape[1])
print("\nData Types:\n", df.dtypes)
print("\nMissing Values:\n", df.isnull().sum())

# Describe numeric columns
print(df.describe())

# Check for duplicates
print("Duplicate Rows:", df.duplicated().sum())

# Plot AQI trend
plt.figure(figsize=(10,5))
sns.lineplot(x='time', y='us_aqi', data=df)
plt.title("Karachi AQI Trend (2023–2025)")
plt.show()

#Checking correlation b/w AQI and Time
plt.figure(figsize=(10,5))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

"""**DATA PRE-PROCESSING -- OUTLIERS DETECTION**"""

# Select only numeric columns (ignoring 'time')
numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns

print(f"Numeric columns found: {list(numeric_cols)}")

# Function to detect outliers using IQR

def detect_outliers_iqr(data, feature):
    Q1 = data[feature].quantile(0.25)
    Q3 = data[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = data[(data[feature] < lower) | (data[feature] > upper)]
    return outliers, lower, upper


# Detect outliers for each numeric feature

for col in numeric_cols:
    outliers, lower, upper = detect_outliers_iqr(df, col)
    print(f"{col}: {len(outliers)} outliers detected | Range = ({lower:.2f}, {upper:.2f})")

#  Visualize outliers using boxplots

plt.figure(figsize=(12, 6))
sns.boxplot(data=df[numeric_cols])
plt.title("Boxplot of Numeric Features Before Outlier Capping")
plt.xticks(rotation=45)
plt.show()

# Handle outliers

df_cleaned = df.copy()
for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[col] = df[col].clip(lower, upper)

print("Outliers have been capped using the IQR method.")

# Before vs After visualization

fig, axes = plt.subplots(2, 1, figsize=(12, 10))

sns.boxplot(data=df[numeric_cols], ax=axes[0])
axes[0].set_title("Before Outlier Capping")
axes[0].tick_params(axis='x', rotation=45)

sns.boxplot(data=df_cleaned[numeric_cols], ax=axes[1])
axes[1].set_title("After Outlier Capping")
axes[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Save cleaned data

OUTPUT_CSV = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-CLEANED.csv"


df.to_csv(OUTPUT_CSV, index=False)

print(" Cleaned dataset saved as 'KARACHI-AQI-RECORDS-2023-2025-CLEANED.csv'")
print(f" Total records: {len(df)}")

# Check if negative values still exist in the cleaned data
cleaned_df = pd.read_csv("AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-CLEANED.csv")
negative_check = (cleaned_df[numeric_cols] < 0).sum()
print("Negative values in cleaned data:")
print(negative_check[negative_check > 0])

# Check for missing values
print(" Missing values:")
print(cleaned_df.isnull().sum())

if cleaned_df.isnull().sum().sum() > 0:
    print(" Missing values found — fixing...")
    cleaned_df = cleaned_df.interpolate(method='time').ffill().bfill()
    print(" Missing values handled.")
else:
    print(" No missing values found.")

#visualizations
def plot_aqi_trends(cleaned_df):
    """Plot AQI trends over time"""
    plt.figure(figsize=(15, 10))

    # Plot 1: US AQI over time
    plt.subplot(2, 2, 1)
    plt.plot(df['time'], df['us_aqi'], alpha=0.7)
    plt.title('US AQI Over Time')
    plt.xlabel('Time')
    plt.ylabel('US AQI')
    plt.xticks(rotation=45)

    # Plot 2: PM2.5 over time
    plt.subplot(2, 2, 2)
    plt.plot(df['time'], df['pm2_5'], alpha=0.7, color='red')
    plt.title('PM2.5 Over Time')
    plt.xlabel('Time')
    plt.ylabel('PM2.5 (μg/m³)')
    plt.xticks(rotation=45)

    # Plot 3: Temperature over time
    plt.subplot(2, 2, 3)
    plt.plot(df['time'], df['temperature_2m'], alpha=0.7, color='orange')
    plt.title('Temperature Over Time')
    plt.xlabel('Time')
    plt.ylabel('Temperature (°C)')
    plt.xticks(rotation=45)

    # Plot 4: Histogram of US AQI
    plt.subplot(2, 2, 4)
    plt.hist(df['us_aqi'], bins=50, alpha=0.7, edgecolor='black')
    plt.title('Distribution of US AQI')
    plt.xlabel('US AQI')
    plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()

def plot_correlations(cleaned_df):
    """Plot correlation matrix of features"""
    # Select numerical columns for correlation
    numerical_cols = ['pm10', 'pm2_5', 'carbon_monoxide', 'nitrogen_dioxide',
                     'sulphur_dioxide', 'ozone', 'us_aqi', 'temperature_2m',
                     'relative_humidity_2m', 'wind_speed_10m']

    correlation_matrix = df[numerical_cols].corr()

    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, fmt='.2f')
    plt.title('Correlation Matrix of AQI and Weather Features')
    plt.tight_layout()
    plt.show()

    # Plot correlation with target (US AQI)
    aqi_correlations = correlation_matrix['us_aqi'].sort_values(ascending=False)
    plt.figure(figsize=(10, 6))
    aqi_correlations.drop('us_aqi').plot(kind='bar')
    plt.title('Feature Correlations with US AQI')
    plt.ylabel('Correlation Coefficient')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_aqi_trends(cleaned_df)
plot_correlations(cleaned_df)

"""**SKEWNESS CHECK AND REMOVAL**"""

df.hist(figsize=(25,10), bins=30)
plt.suptitle("Feature Distributions", fontsize=25)
plt.show()

from sklearn.preprocessing import PowerTransformer
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import skew

numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns

df_before = cleaned_df[numeric_cols].copy()

print("Skewness before:\n", df_before.apply(lambda x: round(skew(x.dropna()), 3)))

# Apply Yeo–Johnson transformation
pt = PowerTransformer(method='yeo-johnson')
df_after = pd.DataFrame(pt.fit_transform(df_before), columns=numeric_cols)

print("\nSkewness after:\n", pd.Series(np.round(skew(df_after, nan_policy='omit'), 3), index=numeric_cols))

plt.figure(figsize=(14, len(numeric_cols)*2.5))

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(len(numeric_cols), 2, i)
    sns.kdeplot(df_before[col], color='red', fill=True, label='Before')
    sns.kdeplot(df_after[col], color='green', fill=True, label='After')
    plt.title(f"{col}: Before vs After ")
    plt.legend()

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(nrows=len(numeric_cols), ncols=2, figsize=(12, len(numeric_cols)*3))

for i, col in enumerate(numeric_cols):
    # Before transformation
    sns.kdeplot(df_before[col], fill=True, color='red', ax=axes[i, 0])
    axes[i, 0].set_title(f"{col} - Before")
    axes[i, 0].set_xlabel('')
    axes[i, 0].set_ylabel('Density')

    # After transformation
    sns.kdeplot(df_after[col], fill=True, color='green', ax=axes[i, 1])
    axes[i, 1].set_title(f"{col} - After")
    axes[i, 1].set_xlabel('')
    axes[i, 1].set_ylabel('Density')

plt.tight_layout()
plt.show()

cleaned_df[numeric_cols] = df_after

transformed_csv = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-TRANSFORMED.csv"
cleaned_df.to_csv(transformed_csv, index=False)
print(" Transformed dataset saved successfully!")

cleaned_df.head()

CSV_PATH = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-CLEANED.csv"
df = pd.read_csv(CSV_PATH)
df.head()

CSV_PATH = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-TRANSFORMED.csv"
df = pd.read_csv(CSV_PATH)
df.head()

"""**FEATURE ENGINEERING**"""

# FEATURE ENGINEERING

CSV_PATH = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-TRANSFORMED.csv"
OUTPUT_CSV = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-ENGINEERED.csv"

df = pd.read_csv(CSV_PATH, parse_dates=['time'])
df.sort_values('time', inplace=True)
df.reset_index(drop=True, inplace=True)

print(f"Total records loaded: {len(df)}")

# BASIC TIME FEATURES

df['hour'] = df['time'].dt.hour
df['day'] = df['time'].dt.day
df['month'] = df['time'].dt.month
df['year'] = df['time'].dt.year
df['day_of_week'] = df['time'].dt.dayofweek
df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
df['season'] = (df['month'] % 12) // 3 + 1  # 1=winter, 2=spring, 3=summer, 4=fall

# CHANGE RATE FEATURES (per hour)

time_diff = df['time'].diff().dt.total_seconds()
df['aqi_change_rate'] = df['us_aqi'].diff() / time_diff * 3600
df['temp_change_rate'] = df['temperature_2m'].diff() / time_diff * 3600
df['humidity_change_rate'] = df['relative_humidity_2m'].diff() / time_diff * 3600
df['wind_change_rate'] = df['wind_speed_10m'].diff() / time_diff * 3600

# LAG FEATURES (past AQI)

df['AQI_lag1'] = df['us_aqi'].shift(1)
df['AQI_lag3'] = df['us_aqi'].shift(3)
df['AQI_lag6'] = df['us_aqi'].shift(6)
df['AQI_lag24'] = df['us_aqi'].shift(24)

# ROLLING FEATURES (trend & stability)

df['AQI_roll3'] = df['us_aqi'].rolling(window=3).mean()
df['AQI_roll6'] = df['us_aqi'].rolling(window=6).mean()
df['AQI_roll24'] = df['us_aqi'].rolling(window=24).mean()
df['AQI_roll_std6'] = df['us_aqi'].rolling(window=6).std()
df['AQI_roll_std24'] = df['us_aqi'].rolling(window=24).std()

# WEATHER ROLLING FEATURES

df['temp_roll6'] = df['temperature_2m'].rolling(window=6).mean()
df['temp_roll24'] = df['temperature_2m'].rolling(window=24).mean()
df['humidity_roll6'] = df['relative_humidity_2m'].rolling(window=6).mean()
df['humidity_roll24'] = df['relative_humidity_2m'].rolling(window=24).mean()
df['wind_roll6'] = df['wind_speed_10m'].rolling(window=6).mean()
df['wind_roll24'] = df['wind_speed_10m'].rolling(window=24).mean()

# POLLUTANT RATIOS

df['pm25_ratio'] = df['pm2_5'] / (df['pm10'] + 1e-5)
df['no2_pm_ratio'] = df['nitrogen_dioxide'] / (df['pm2_5'] + 1e-5)
df['co_no2_ratio'] = df['carbon_monoxide'] / (df['nitrogen_dioxide'] + 1e-5)
df['so2_pm_ratio'] = df['sulphur_dioxide'] / (df['pm2_5'] + 1e-5)
df['so2_no2_ratio'] = df['sulphur_dioxide'] / (df['nitrogen_dioxide'] + 1e-5)


# INTERACTION FEATURES

df['temp_aqi_interaction'] = df['temperature_2m'] * df['us_aqi']
df['humidity_aqi_interaction'] = df['relative_humidity_2m'] * df['us_aqi']

# AQI TRENDS (difference over hours)

df['AQI_trend_6h'] = df['us_aqi'] - df['us_aqi'].shift(6)
df['AQI_trend_24h'] = df['us_aqi'] - df['us_aqi'].shift(24)

# TARGET VARIABLES (forecast horizons)

df['target_aqi_24h'] = df['us_aqi'].shift(-24)
df['target_aqi_48h'] = df['us_aqi'].shift(-48)
df['target_aqi_72h'] = df['us_aqi'].shift(-72)


# CLEANUP & SAVE
df = df.dropna().reset_index(drop=True)
df.to_csv(OUTPUT_CSV, index=False)
print(f"All derived features added successfully and saved to {OUTPUT_CSV}")
print("Final feature columns:", len(df.columns))
print("Last few rows:")
print(df.tail(3))

print("Missing values:")
print(df.isnull().sum())

"""**FEATURE CORRELATION W.R.T TARGET VARIABLES**"""

features_df = df.drop(columns=['time', 'target_aqi_24h', 'target_aqi_48h', 'target_aqi_72h'])

target_cols = ['target_aqi_24h', 'target_aqi_48h', 'target_aqi_72h']

corr_targets = features_df.corrwith(df['target_aqi_24h']).to_frame(name='target_aqi_24h')
for target in ['target_aqi_48h', 'target_aqi_72h']:
    corr_targets[target] = features_df.corrwith(df[target])

corr_targets = corr_targets.sort_values(by='target_aqi_72h', ascending=False)

plt.figure(figsize=(10, 12))
sns.heatmap(
    corr_targets,
    annot=True,
    cmap='coolwarm',
    fmt=".2f"
)
plt.title("Feature Correlation with AQI Targets (24h, 48h, 72h)")
plt.show()

# Drop non-numeric/time columns AND target columns
numeric_df = df.drop(columns=['time','target_aqi_24h','target_aqi_48h','target_aqi_72h'])

# Target columns
target_cols = ['target_aqi_24h','target_aqi_48h','target_aqi_72h']

# Compute correlation of features with targets
corr_df = numeric_df.corrwith(df['target_aqi_24h']).to_frame(name='target_aqi_24h')
for target in ['target_aqi_48h','target_aqi_72h']:
    corr_df[target] = numeric_df.corrwith(df[target])

# Prepare for seaborn (long format)
corr_long = corr_df.reset_index().melt(
    id_vars='index',
    value_vars=target_cols,
    var_name='Target',
    value_name='Correlation'
)
corr_long.rename(columns={'index': 'Feature'}, inplace=True)

# Plot grouped bar chart
plt.figure(figsize=(10, 15))
sns.barplot(
    data=corr_long,
    x='Correlation',
    y='Feature',
    hue='Target',
    dodge=True,
    palette='coolwarm'
)
plt.title("Feature Correlation with AQI Targets (24h, 48h, 72h)", fontsize=14)
plt.xlabel("Correlation Coefficient", fontsize=12)
plt.ylabel("Features", fontsize=12)
plt.grid(True, axis='x', linestyle='--', alpha=0.7)
plt.legend(title='Target')
plt.show()

def top_features_correlation(df, target_cols, drop_cols=['time'], top_n=5):
    # Drop non-feature columns (time + target columns)
    feature_cols = df.drop(columns=drop_cols + target_cols)

    # Compute correlation of each feature with each target
    corr_df = pd.DataFrame()
    for target in target_cols:
        corr_df[target] = feature_cols.corrwith(df[target])

    # Prepare long format for seaborn
    corr_long = corr_df.reset_index().melt(
        id_vars='index',
        value_vars=target_cols,
        var_name='Target',
        value_name='Correlation'
    )
    corr_long.rename(columns={'index': 'Feature'}, inplace=True)

    # Add absolute correlation column for sorting
    corr_long['abs_corr'] = corr_long['Correlation'].abs()

    # Get top N features per target
    top_features = corr_long.sort_values(['Target','abs_corr'], ascending=[True, False]).groupby('Target').head(top_n)

    # Plot
    plt.figure(figsize=(12, 8))
    sns.barplot(
        data=top_features,
        x='Correlation',
        y='Feature',
        hue='Target',
        dodge=True,
        palette='coolwarm'
    )
    plt.title(f"Top {top_n} Features Correlated with AQI Targets")
    plt.xlabel("Correlation Coefficient")
    plt.ylabel("Features")
    plt.grid(True, axis='x', linestyle='--', alpha=0.7)
    plt.legend(title='Target')
    plt.tight_layout()
    plt.show()

    # Best features across all targets combined (unique features)
    top_features_combined = top_features['Feature'].unique().tolist()

    print("Top features for all 3 targets combined:")
    print(top_features_combined)

    return top_features_combined

df = pd.read_csv("AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-ENGINEERED.csv")
target_cols = ['target_aqi_24h','target_aqi_48h','target_aqi_72h']
best_features = top_features_correlation(df, target_cols, drop_cols=['time'], top_n=15)

"""**HOPSWORKS CONNECTION**"""

import hopsworks

# Connecting to Hopsworks
project = hopsworks.login(api_key_value="HOSPWORKS_API_KEY_HERE")
fs = project.get_feature_store()

print("Connected to Hopsworks project:", project.name)

"""**FEATURE GROUP -- OFFLINE VERSION FOR MODEL TRAINING**"""

# CURRENT WORKING FEATURE GROUP TRANSFORMED AND ENGINEERED

data_path = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-ENGINEERED.csv"
df = pd.read_csv(data_path)
df['time'] = pd.to_datetime(df['time'])

features_df = df.drop(columns=['target_aqi_24h', 'target_aqi_48h', 'target_aqi_72h'], errors='ignore')
fg_name = "aqi_features_engineered"
fg_version = 1

try:
    aqi_fg = fs.get_feature_group(name=fg_name, version=fg_version)
    print(f"Existing feature group found: {fg_name}_v{fg_version}")
except:
    aqi_fg = fs.create_feature_group(
        name=fg_name,
        version=fg_version,
        description="Engineered Karachi AQI features (after skewness correction)",
        primary_key=["time"],
        event_time="time",
        online_enabled=False
        )
print("Feature group ready:", aqi_fg)

# UPDATING HOPSWORKS WITH NEW RECORDS
import hopsworks
import pandas as pd

# Connect to Hopsworks
project = hopsworks.login()
fs = project.get_feature_store()

# Load your new CSV
data_path = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-ENGINEERED.csv"
df = pd.read_csv(data_path)
df['time'] = pd.to_datetime(df['time'])

# Removing target variables
features_df = df.drop(columns=['target_aqi_24h', 'target_aqi_48h', 'target_aqi_72h'], errors='ignore')

# Define feature group info
fg_name = "aqi_features_engineered"
fg_version = 1

def upload_to_hopsworks_safe(features_df):
    """
    Safely upload engineered data to Hopsworks Feature Store.
    Skips duplicates based on primary key 'time'.
    """
    # Try to get or create feature group
    try:
        aqi_fg = fs.get_feature_group(name=fg_name, version=fg_version)
        print(f"Existing feature group found: {fg_name}_v{fg_version}")
    except:
        aqi_fg = fs.create_feature_group(
            name=fg_name,
            version=fg_version,
            description="Engineered Karachi AQI features (after skewness correction)",
            primary_key=["time"],
            event_time="time",
            online_enabled=False
        )
        print(f"Created new feature group: {fg_name}_v{fg_version}")

    # Read existing timestamps
    try:
        existing_times = aqi_fg.read(online=False)["time"].tolist()
    except Exception:
        existing_times = []

    # Filter out duplicates
    new_records = features_df[~features_df["time"].isin(existing_times)]
    if len(new_records) == 0:
        print("No new records to upload. All data already exists in Hopsworks.")
        return

    # Insert only new data
    aqi_fg.insert(new_records)
    print(f"{len(new_records)} new records uploaded to Hopsworks safely!")

#  Run function
upload_to_hopsworks_safe(features_df)

"""**FETCHING DATA FROM FEATURE GROUP**"""

fg_name = "aqi_features_engineered"
fg_version = 1

try:
    aqi_fg = fs.get_feature_group(name=fg_name, version=fg_version)
    print(f"Existing feature group found: {fg_name}_v{fg_version}")
except:
    aqi_fg = fs.create_feature_group(
        name=fg_name,
        version=fg_version,
        description="Engineered Karachi AQI features (after skewness correction)",
        primary_key=["time"],
        event_time="time",
        online_enabled=False
        )
print("Feature group ready:", aqi_fg)

df_features = aqi_fg.read()
# first few rows
df_features = df_features.sort_values(by="time")
df_features.head()

df_features.tail()

"""**GETTING FEATURES FROM HOSPWORKS AND TARGET VARIABLE FROM CSV**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns

# Loading Features from hopsworks and target columns from csv

features_df = aqi_fg.read()
print(" Features loaded from Hopsworks:", features_df.shape)

csv_path = "AQI_Project/data/KARACHI-AQI-RECORDS-2023-2025-ENGINEERED.csv"
targets_df = pd.read_csv(csv_path, usecols=["time", "target_aqi_24h", "target_aqi_48h", "target_aqi_72h"])
targets_df['time'] = pd.to_datetime(targets_df['time']).dt.tz_localize(None)

# Merging based on time column
features_df['time'] = pd.to_datetime(features_df['time']).dt.tz_localize(None)
merged_df = pd.merge(features_df, targets_df, on="time", how="inner")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

# Splitting Features and Targets

X = merged_df.drop(columns=['time', 'target_aqi_24h', 'target_aqi_48h', 'target_aqi_72h'])
y = merged_df[['target_aqi_24h', 'target_aqi_48h', 'target_aqi_72h']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**FEATURE SELECTION USING MODEL**"""

# Random Forest Model
rf_fs = RandomForestRegressor(n_estimators=200, random_state=42)
rf_fs.fit(X_train, y_train)

importances = pd.DataFrame(rf_fs.feature_importances_, index=X.columns, columns=['importance']).sort_values(by='importance', ascending=False)
print("Top Features:\n", importances.head(15))

# Plot top 15 features
plt.figure(figsize=(10,8))
sns.barplot(x='importance', y=importances.head(15).index, data=importances.head(15))
plt.title("Top 15 Important Features (Random Forest)")
plt.show()

rf_fs = RandomForestRegressor(n_estimators=200, random_state=42)
rf_fs.fit(X_train, y_train)

rf_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_fs.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("Top 15 Features:\n", rf_importance_df.head(15))

feature_counts = [10, 15, 20, 25, 30, 35, X_train.shape[1]]

models = {
    'RandomForest': RandomForestRegressor(n_estimators=200, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=200, learning_rate=0.1, random_state=42),
    'LightGBM': LGBMRegressor(n_estimators=200, learning_rate=0.1, random_state=42)
}

results = []

for n in feature_counts:
    top_features = rf_importance_df['Feature'].head(n)
    X_train_sel = X_train[top_features]
    X_test_sel = X_test[top_features]

    for name, model_base in models.items():
        model = MultiOutputRegressor(model_base)
        model.fit(X_train_sel, y_train)
        y_pred = model.predict(X_test_sel)

        test_r2 = r2_score(y_test, y_pred, multioutput='uniform_average')
        rmse = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='uniform_average'))
        mae = mean_absolute_error(y_test, y_pred, multioutput='uniform_average')

        results.append({
            'Model': name,
            'Top_Features': n,
            'R2': test_r2,
            'RMSE': rmse,
            'MAE': mae
        })

# RESULTS COMPARISON
results_df = pd.DataFrame(results)
results_df = results_df.sort_values(by=['Model','R2'], ascending=[True, False])
print("\n Top-N Feature Optimization Results for All Models:\n")
print(results_df)

# Best model & feature count
best_row = results_df.loc[results_df['R2'].idxmax()]
print(f"\n Best Model: {best_row['Model']} with Top {best_row['Top_Features']} features, R2 = {best_row['R2']:.4f}")

"""**FEATURES** **SELECTED:** **15**"""

rf_fs = RandomForestRegressor(n_estimators=200, random_state=42)
rf_fs.fit(X_train, y_train)

rf_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_fs.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("Top 15 Features:\n", rf_importance_df.head(15))

# REMOVE COMMENT AND RUN THIS CELL IF ONLY HAVE TO USE THESE VARIABLES WITHOUT TRAINING MODEL AGAIN ( EG: LOADING SAVED MODEL AND RUNNING SHAP ANALYSIS)
#top_features = [f for f in rf_importance_df['Feature'].head(15) if f in X_train.columns]

#X_train_sel = X_train[top_features].copy()
#X_test_sel = X_test[top_features].copy()

#print("Selected top features:", top_features)
#print("X_train_sel shape:", X_train_sel.shape)

"""**MODEL TRAINING USING TOP 15 FEATURES**"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error


# Select top 15 features from previous RF importance

top_features = rf_importance_df['Feature'].head(15)
X_train_sel = X_train[top_features]
X_test_sel = X_test[top_features]


# Define models

models = {
    'RandomForest': RandomForestRegressor(n_estimators=200, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=200, learning_rate=0.1, random_state=42, verbosity=0),
    'LightGBM': LGBMRegressor(n_estimators=200, learning_rate=0.1, random_state=42)
}

# Train models and evaluate

results = []

for name, model_base in models.items():
    # Multi-output regression
    model = MultiOutputRegressor(model_base)
    model.fit(X_train_sel, y_train)

    # Predictions
    y_train_pred = model.predict(X_train_sel)
    y_test_pred = model.predict(X_test_sel)

    # Metrics
    train_r2 = r2_score(y_train, y_train_pred, multioutput='uniform_average')
    test_r2 = r2_score(y_test, y_test_pred, multioutput='uniform_average')
    train_mse = mean_squared_error(y_train, y_train_pred, multioutput='uniform_average')
    test_mse = mean_squared_error(y_test, y_test_pred, multioutput='uniform_average')
    train_mae = mean_absolute_error(y_train, y_train_pred, multioutput='uniform_average')
    test_mae = mean_absolute_error(y_test, y_test_pred, multioutput='uniform_average')

    results.append({
        'Model': name,
        'Train R2': round(train_r2, 4),
        'Test R2': round(test_r2, 4),
        'Train MSE': round(train_mse, 4),
        'Test MSE': round(test_mse, 4),
        'Train MAE': round(train_mae, 4),
        'Test MAE': round(test_mae, 4)
    })

# Display results in a table

results_df = pd.DataFrame(results)
results_df = results_df[['Model', 'Train R2', 'Test R2', 'Train MSE', 'Test MSE', 'Train MAE', 'Test MAE']]

print("\nModel Performance with Top 15 Features:\n")
print(results_df)

# Highlight the best model based on Test R2
best_row = results_df.loc[results_df['Test R2'].idxmax()]
print(f"\nBest Model: {best_row['Model']} with Test R2 = {best_row['Test R2']:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns

# Set seaborn style
sns.set(style="whitegrid")

# 1️⃣ Plot R2 Scores
plt.figure(figsize=(10, 5))
r2_df = results_df.melt(id_vars='Model', value_vars=['Train R2', 'Test R2'], var_name='Dataset', value_name='R2')
sns.barplot(data=r2_df, x='Model', y='R2', hue='Dataset', palette='Set2')
plt.title('Train vs Test R2 Score for Models')
plt.ylim(0, 1)
plt.show()

#  Plot MSE
plt.figure(figsize=(10, 5))
mse_df = results_df.melt(id_vars='Model', value_vars=['Train MSE', 'Test MSE'], var_name='Dataset', value_name='MSE')
sns.barplot(data=mse_df, x='Model', y='MSE', hue='Dataset', palette='Set1')
plt.title('Train vs Test MSE for Models')
plt.show()

#  Plot MAE
plt.figure(figsize=(10, 5))
mae_df = results_df.melt(id_vars='Model', value_vars=['Train MAE', 'Test MAE'], var_name='Dataset', value_name='MAE')
sns.barplot(data=mae_df, x='Model', y='MAE', hue='Dataset', palette='Set3')
plt.title('Train vs Test MAE for Models')
plt.show()

# Optional: Highlight best model visually
best_model_name = best_row['Model']
print(f"Best model based on Test R2: {best_model_name}")

"""**CV AND FINDING BEST HYPER PARAMETERS**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV # Import GridSearchCV here
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error


# Split Features and Targets

X = merged_df.drop(columns=['time', 'target_aqi_24h', 'target_aqi_48h', 'target_aqi_72h'])
y = merged_df[['target_aqi_24h', 'target_aqi_48h', 'target_aqi_72h']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Train shape:", X_train.shape, y_train.shape)
print("Test shape: ", X_test.shape, y_test.shape)

# Select top features from RF importance

top_features = [f for f in rf_importance_df['Feature'].head(15) if f in X_train.columns]

X_train_sel = X_train[top_features].copy()
X_test_sel = X_test[top_features].copy()

print("Selected top features:", top_features)
print("X_train_sel shape:", X_train_sel.shape)

# Define hyperparameter grids

rf_params = {
    'estimator__n_estimators': [100, 200],
    'estimator__max_depth': [10, 20, None],
    'estimator__min_samples_split': [2, 5],
    'estimator__min_samples_leaf': [1, 2],
    'estimator__max_features': ['sqrt']
}

xgb_params = {
    'estimator__n_estimators': [200, 300],
    'estimator__learning_rate': [0.05, 0.1],
    'estimator__max_depth': [3, 5, 7],
    'estimator__subsample': [0.8, 1],
    'estimator__colsample_bytree': [0.8, 1],
    'estimator__reg_lambda': [1, 5],
    'estimator__reg_alpha': [0, 0.1]
}

lgb_params = {
    'estimator__n_estimators': [200, 300],
    'estimator__learning_rate': [0.05, 0.1],
    'estimator__max_depth': [3, 5, 7, -1],
    'estimator__num_leaves': [31, 50, 70],
    'estimator__subsample': [0.8, 1],
    'estimator__colsample_bytree': [0.8, 1],
    'estimator__reg_lambda': [1, 5],
    'estimator__reg_alpha': [0, 0.1]
}

# GridSearchCV for MultiOutput models

# Random Forest
rf_grid = GridSearchCV(
    MultiOutputRegressor(RandomForestRegressor(random_state=42)),
    rf_params,
    cv=3,
    scoring='r2',
    n_jobs=-1
)
rf_grid.fit(X_train_sel, y_train)

print("\nBest Random Forest Params:", rf_grid.best_params_)
print("Best RF Cross-Validated R²:", rf_grid.best_score_)

# XGBoost
xgb_grid = GridSearchCV(
    MultiOutputRegressor(XGBRegressor(random_state=42, objective='reg:squarederror', verbosity=0)),
    xgb_params,
    cv=3,
    scoring='r2',
    n_jobs=-1
)
xgb_grid.fit(X_train_sel, y_train)

print("\nBest XGBoost Params:", xgb_grid.best_params_)
print("Best XGB Cross-Validated R²:", xgb_grid.best_score_)

# LightGBM
lgb_grid = GridSearchCV(
    MultiOutputRegressor(LGBMRegressor(random_state=42)),
    lgb_params,
    cv=3,
    scoring='r2',
    n_jobs=-1
)
lgb_grid.fit(X_train_sel, y_train)

print("\nBest LightGBM Params:", lgb_grid.best_params_)
print("Best LightGBM Cross-Validated R²:", lgb_grid.best_score_)

# Compare models

cv_scores = {
    'RandomForest': rf_grid.best_score_,
    'XGBoost': xgb_grid.best_score_,
    'LightGBM': lgb_grid.best_score_
}
best_model_name = max(cv_scores, key=cv_scores.get)
print(f"\nBest model based on CV R²: {best_model_name}")

# Train final model & evaluate

if best_model_name == 'RandomForest':
    final_model = rf_grid.best_estimator_
elif best_model_name == 'XGBoost':
    final_model = xgb_grid.best_estimator_
else:
    final_model = lgb_grid.best_estimator_

final_model.fit(X_train_sel, y_train)

train_preds = final_model.predict(X_train_sel)
test_preds = final_model.predict(X_test_sel)

train_r2 = r2_score(y_train, train_preds, multioutput='uniform_average')
test_r2 = r2_score(y_test, test_preds, multioutput='uniform_average')
train_mse = mean_squared_error(y_train, train_preds, multioutput='uniform_average')
test_mse = mean_squared_error(y_test, test_preds, multioutput='uniform_average')
train_mae = mean_absolute_error(y_train, train_preds, multioutput='uniform_average')
test_mae = mean_absolute_error(y_test, test_preds, multioutput='uniform_average')

print(f"\n {best_model_name} Performance (After Hyperparameter Tuning):")
print(f"Train R²: {train_r2:.4f}")
print(f"Test R²:  {test_r2:.4f}")
print(f"Train MSE: {train_mse:.4f}")
print(f"Test MSE:  {test_mse:.4f}")
print(f"Train MAE: {train_mae:.4f}")
print(f"Test MAE:  {test_mae:.4f}")
print(f"R² Difference (Train-Test): {abs(train_r2 - test_r2):.4f}")

"""**BEST MODEL - LIGHT GBM**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Predicted vs Actual (for each AQI target)

target_names = ['target_aqi_24h', 'target_aqi_48h', 'target_aqi_72h']

fig, axes = plt.subplots(1, 3, figsize=(18, 5))
for i, target in enumerate(target_names):
    sns.scatterplot(x=y_test.iloc[:, i], y=test_preds[:, i], ax=axes[i], s=40)
    axes[i].set_title(f'Predicted vs Actual - {target}')
    axes[i].set_xlabel('Actual AQI')
    axes[i].set_ylabel('Predicted AQI')
    # 45-degree reference line
    min_val = min(y_test.iloc[:, i].min(), test_preds[:, i].min())
    max_val = max(y_test.iloc[:, i].max(), test_preds[:, i].max())
    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--')
plt.tight_layout()
plt.show()

#  Feature Importance (from LightGBM)

# Extract feature importances from one LightGBM estimator
if hasattr(final_model.estimators_[0], 'feature_importances_'):
    importances = final_model.estimators_[0].feature_importances_
    feat_imp_df = pd.DataFrame({'Feature': X_train_sel.columns, 'Importance': importances})
    feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(data=feat_imp_df.head(15), x='Importance', y='Feature', palette='viridis')
    plt.title('Top 15 Feature Importances (LightGBM)')
    plt.xlabel('Importance Score')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()
else:
    print(" Feature importances not available for this model type.")

#  Residual (Error) Distribution

residuals = y_test.values - test_preds
residuals_flat = residuals.flatten()

plt.figure(figsize=(8, 5))
sns.histplot(residuals_flat, bins=30, kde=True, color='skyblue')
plt.title('Residuals Distribution (Actual - Predicted)')
plt.xlabel('Prediction Error')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()


#  Optional: R² Comparison Chart

r2_scores = {
    'Train R²': train_r2,
    'Test R²': test_r2
}

plt.figure(figsize=(6, 5))
sns.barplot(x=list(r2_scores.keys()), y=list(r2_scores.values()), palette='coolwarm')
plt.title(f'R² Comparison ({best_model_name})')
plt.ylabel('R² Score')
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

"""**PERFORMING CV ON TOP 20 FEATURES FOR COMPARISON**


"""

feature_counts = [15, 20]
results_list = []

for n in feature_counts:
    top_features = [f for f in rf_importance_df['Feature'].head(n) if f in X_train.columns]
    X_train_sel = X_train[top_features].copy()
    X_test_sel = X_test[top_features].copy()

    print(f"\n Testing LightGBM with Top {n} Features")

    lgb_grid = GridSearchCV(
        MultiOutputRegressor(LGBMRegressor(random_state=42)),
        lgb_params,
        cv=3,
        scoring='r2',
        n_jobs=-1
    )
    lgb_grid.fit(X_train_sel, y_train)

    best_lgb = lgb_grid.best_estimator_
    best_lgb.fit(X_train_sel, y_train)
    preds = best_lgb.predict(X_test_sel)

    test_r2 = r2_score(y_test, preds, multioutput='uniform_average')
    test_mse = mean_squared_error(y_test, preds, multioutput='uniform_average')
    test_mae = mean_absolute_error(y_test, preds, multioutput='uniform_average')

    results_list.append({
        'Features': n,
        'Best CV R²': lgb_grid.best_score_,
        'Test R²': round(test_r2, 4),
        'Test MSE': round(test_mse, 4),
        'Test MAE': round(test_mae, 4)
    })

# Convert to DataFrame for easy viewing
results_df = pd.DataFrame(results_list)
print("\n LightGBM Feature Comparison:\n")
print(results_df)

"""**FINAL MODEL: LIGHT GBM**

**FEATURES: 15** [pm2_5, us_aqi, temp_roll24, day, month, humidity_roll24, wind_roll24, aqi_roll24, aqi_lag24, aqi_roll_std24, day_of_week, pm10, year, aqi_trend_24h, aqi_roll3]

**Best** **LightGBM** **Params**: {'estimator__colsample_bytree': 1, 'estimator__learning_rate': 0.1, 'estimator__max_depth': -1, 'estimator__n_estimators': 300, 'estimator__num_leaves': 70, 'estimator__reg_alpha': 0.1, 'estimator__reg_lambda': 1, 'estimator__subsample': 0.8}

**SAVE BEST MODEL**
"""

import joblib

# Save the final trained model
joblib.dump(final_model, "best_lightgbm_multioutput.pkl")
print(" Model saved successfully!")

from google.colab import files
files.download("best_lightgbm_multioutput.pkl")

"""**SHAP ANALYSIS**"""

import shap
import matplotlib.pyplot as plt
import pandas as pd

"""**SHAP PLOT FOR LIGHT GBM MODEL WITH 15 FEATURES**"""

import joblib
import shap
import matplotlib.pyplot as plt

# --- Loadig Saved model ---
final_model = joblib.load("AQI_Project/models/best_lightgbm_multioutput.pkl")

# ---  Recreate X_train_sel and X_test_sel using top_features ---

X_train_sel = X_train[top_features].copy()
X_test_sel = X_test[top_features].copy()

print("Selected top features:", top_features)
print("X_train_sel shape:", X_train_sel.shape)

# --- SHAP Analysis ---
target_names = ["target_aqi_24h", "target_aqi_48h", "target_aqi_72h"]

print("\nPerforming SHAP analysis for each AQI target...")

for i, target_name in enumerate(target_names):
    print(f"\n--- SHAP Analysis for {target_name} ---")

    # Get the i-th LightGBM model from MultiOutputRegressor
    single_model = final_model.estimators_[i]

    # Create SHAP explainer
    explainer = shap.Explainer(single_model, X_train_sel)
    shap_values = explainer(X_test_sel, check_additivity=False)

    # Summary Plot
    print(f"\nSHAP Summary Plot - {target_name}")
    shap.summary_plot(shap_values, X_test_sel)

    # Bar Plot (average feature importance)
    print(f"\nSHAP Feature Importance Bar Plot - {target_name}")
    shap.plots.bar(shap_values)

    plt.show()

print("\n SHAP analysis completed and visualized successfully!")

"""**DOWNLOADING SHAP PLOTS**"""

for i, target_name in enumerate(target_names):
    print(f"\n--- SHAP Analysis for {target_name} ---")

    single_model = final_model.estimators_[i]
    explainer = shap.Explainer(single_model, X_train_sel)
    shap_values = explainer(X_test_sel, check_additivity=False)

    # --- Summary Plot ---
    shap.summary_plot(shap_values, X_test_sel, show=False)
    plt.tight_layout()
    plt.savefig(f"shap_plots/summary_{target_name}.png")  # Save summary plot
    plt.close()  # Close to avoid overlap

    # --- Bar Plot ---
    shap.plots.bar(shap_values, show=False)
    plt.tight_layout()
    plt.savefig(f"shap_plots/bar_{target_name}.png")  # Save bar plot
    plt.close()

import numpy
import shap
import confluent_kafka
import hopsworks
import sys

print("Python version:", sys.version)
print("NumPy version:", numpy.__version__)
print("SHAP version:", shap.__version__)
print("Hopsworks version:", hopsworks.__version__)
print("Confluent Kafka version:", confluent_kafka.__version__)

!pip show openmeteo-requests

import openmeteo_requests
import requests_cache
import requests

print("requests_cache version:", requests_cache.__version__)
print("requests version:", requests.__version__)

!pip show pandas